import csv
import re
from pathlib import Path
from collections import defaultdict
import yaml

# ========== ĞĞĞ¡Ğ¢Ğ ĞĞ™ĞšĞ˜ ==========
vault_path = Path(r"C:\Users\stasr\Documents\Obsidian Vault\obsidian-valut")
parent_note_relpath = Path(r"Ğ¢ĞµĞ³Ğ¸\ĞĞ°Ğ²Ñ‹ĞºĞ¸, Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ\__ĞĞ°Ğ²Ñ‹ĞºĞ¸, Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ°, Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ñ.md")
allow_dirs = {
    "Ğ¢ĞµĞ³Ğ¸",
    "_ĞŸĞ»Ğ°Ğ½Ğ¸Ñ€Ğ¾Ğ²Ñ‰Ğ¸Ğº",
    "_WiKi",
}
max_depth = 5
yaml_stop_keys = {"Ğ¢Ğ¸Ğ¿-Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸"}
# ===============================

parent_note_stem = parent_note_relpath.stem
link_pattern = re.compile(r"\[\[([^\[\]|]+)(?:\|[^\]]*)?\]\]")

link_map = defaultdict(set)
note_paths = {}
stop_nodes = set()

# ğŸ‘‡ Ğ¿Ğ°Ñ€ÑĞ¸Ğ¼ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾, Ñ Ñ€ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ YAML (Ğ½Ğµ Ñ‡ĞµÑ€ĞµĞ· frontmatter)
for md_file in vault_path.rglob("*.md"):
    rel_path = md_file.relative_to(vault_path)
    if not any(part in allow_dirs for part in rel_path.parts):
        continue

    try:
        content = md_file.read_text(encoding='utf-8')
    except Exception as e:
        print(f"âš ï¸ ĞŸÑ€Ğ¾Ğ¿ÑƒÑ‰ĞµĞ½ Ñ„Ğ°Ğ¹Ğ» {rel_path} â€” Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ñ‡Ñ‚ĞµĞ½Ğ¸Ñ: {e}")
        continue

    from_note = md_file.stem
    note_paths[from_note] = rel_path

    # ğŸ§  ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ YAML, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ
    if content.startswith('---'):
        try:
            yaml_text = content.split('---', 2)[1]
            metadata = yaml.safe_load(yaml_text)
            if isinstance(metadata, dict) and any(k in metadata for k in yaml_stop_keys):
                stop_nodes.add(from_note)
        except Exception:
            pass  # Ğ½Ğµ Ğ¼ĞµÑˆĞ°ĞµĞ¼, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ YAML

    # ğŸ“ Ğ˜Ñ‰ĞµĞ¼ Ğ²ÑĞµ [[Ğ¡ÑÑ‹Ğ»ĞºĞ¸]]
    links = link_pattern.findall(content)
    for to_note in links:
        link_map[to_note].add(from_note)

# ğŸ“ Ğ ĞµĞºÑƒÑ€ÑĞ¸Ğ²Ğ½Ğ¾ ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºĞ¸ ÑÑÑ‹Ğ»Ğ¾Ğº
def collect_paths(current, path, depth, results):
    if depth >= max_depth:
        results.append(path)
        return
    if current in stop_nodes:
        results.append(path)
        return

    backlinks = link_map.get(current, [])
    if not backlinks:
        results.append(path)
    else:
        for b in backlinks:
            if b in path:
                continue
            collect_paths(b, path + [b], depth + 1, results)

# ğŸš€ Ğ¡Ñ‚Ğ°Ñ€Ñ‚
all_paths = []
collect_paths(parent_note_stem, [parent_note_stem], 0, all_paths)

# ğŸ’¾ Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ² CSV
output_csv = "link_hierarchy.csv"
with open(output_csv, 'w', newline='', encoding='utf-8') as f:
    writer = csv.writer(f)
    header = [f"Level {i}" for i in range(max_depth + 1)]
    writer.writerow(header)
    for path in all_paths:
        row = path + [''] * (max_depth + 1 - len(path))
        writer.writerow(row)

print(f"âœ… CSV-Ñ„Ğ°Ğ¹Ğ» ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½: {output_csv}")
